{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "#Make the dictionary from sent_dic_ru file\n",
    "with open('sent_dic_ru.txt') as f:\n",
    "    sent_learn = f.readlines()\n",
    "sent_learn = [x.strip('\\n') for x in sent_learn]                \n",
    "#print(sent_learn)\n",
    "\n",
    "def norm(word):    \n",
    "    p_obj = morph.parse(word)\n",
    "    try:\n",
    "        p = p_obj[0]\n",
    "        return p.normal_form, p.tag.POS\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sent_learn_prep(sent):\n",
    "    #Normalization and stop-words delete from sent_list\n",
    "    sent_plus = []    \n",
    "    label_plus = []\n",
    "    sent_minus = []    \n",
    "    label_minus = []\n",
    "\n",
    "    sent_list = []\n",
    "    for x in sent:      \n",
    "        #Split into words\n",
    "        str_list = x.split()\n",
    "\n",
    "        #List from sentenses (words in list)\n",
    "        sent_list.append(str_list)\n",
    "\n",
    "    for sent in sent_list:\n",
    "        #print(sent)\n",
    "        one_sent = ''\n",
    "\n",
    "        if abs(float(sent[-1])) > 0.3:\n",
    "            for x in sent[:-1]:\n",
    "                #print(x)\n",
    "                x_norm = norm(x) \n",
    "                if x_norm is not None:\n",
    "                    if x_norm[1] not in ['PREP', 'PRCL', 'CONJ', 'NPRO']:\n",
    "                        one_sent = one_sent + ' ' + x_norm[0]                \n",
    "            weight = float(sent[-1])\n",
    "            if weight > 0:\n",
    "                weight_norm = 1\n",
    "            else:\n",
    "                weight_norm = -1\n",
    "            one_sent = one_sent.lstrip()\n",
    "            #print(one_sent)\n",
    "            if weight_norm > 0:                \n",
    "                sent_plus.append(one_sent)\n",
    "                label_plus.append(weight_norm)\n",
    "            else:\n",
    "                sent_minus.append(one_sent)\n",
    "                label_minus.append(weight_norm)\n",
    "    #print(sent_list_norm)\n",
    "    #print(label_list)\n",
    "    return (sent_plus, label_plus), (sent_minus, label_minus)\n",
    "\n",
    "sent_p, sent_m = sent_learn_prep(sent_learn)\n",
    "#print(sent_p, sent_m )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('russian')\n",
    "#print(stopWords)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectoriz_p = CountVectorizer(stop_words = stopWords)\n",
    "vectoriz_m = CountVectorizer(stop_words = stopWords)\n",
    "\n",
    "#Bags of words making\n",
    "sent_vect_p = vectoriz_p.fit_transform(sent_p[0])\n",
    "sent_vect_m = vectoriz_m.fit_transform(sent_m[0])\n",
    "\n",
    "#print(sent_vect_p)\n",
    "#print(sent_vect_m)\n",
    "#print(sent_vect_p.shape)\n",
    "#print(sent_vect_m.shape)\n",
    "#print(vectoriz_p.vocabulary_)\n",
    "#print(vectoriz_m.vocabulary_)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "words_p = vectoriz_p.get_feature_names()\n",
    "counts_p = np.asarray(sent_vect_p.sum(axis=0)).ravel()\n",
    "#words_counts_p = map(lambda x,y: (x,y), words_p, counts_p)\n",
    "#words_counts_p = zip(words_p, counts_p)\n",
    "#print(*words_counts_p)\n",
    "words_counts_p = dict(zip(words_p, counts_p))\n",
    "#print(words_counts_p)\n",
    "\n",
    "words_m = vectoriz_m.get_feature_names()\n",
    "counts_m = np.asarray(sent_vect_m.sum(axis=0)).ravel()\n",
    "#words_counts_p = map(lambda x,y: (x,y), words_p, counts_p)\n",
    "#words_counts_m = zip(words_m, counts_m)\n",
    "#print(*words_counts_m)\n",
    "words_counts_m = dict(zip(words_m, counts_m))\n",
    "#print(words_counts_m)\n",
    "\n",
    "#Intersection between two sets (with +1 and -1 sentiment)\n",
    "words_intersect = set(words_p).intersection(set(words_m))\n",
    "#print(words_intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Expanding the list of stop words\n",
    "for x in words_intersect:\n",
    "    if abs(words_counts_p[x] - words_counts_m[x]) <= 1:\n",
    "        stopWords.append(x)\n",
    "#print(stopWords)\n",
    "with open ('stopWords.txt', 'w') as fp:\n",
    "    for x in stopWords:\n",
    "        fp.write(\"%s\\n\" % (x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Making the joined bag of words\n",
    "vectoriz = CountVectorizer(stop_words = stopWords, min_df = 0.001, max_df = 0.999)\n",
    "sent_vect = vectoriz.fit_transform(sent_p[0] + sent_m[0])\n",
    "#List of words - coordinates of the vector\n",
    "words = vectoriz.get_feature_names()\n",
    "counts = np.asarray(sent_vect.sum(axis=0)).ravel()\n",
    "words_counts = dict(zip(words, counts))\n",
    "#print(words_counts)\n",
    "with open ('words_count.txt', 'w') as fp:\n",
    "    for x in words_counts.keys():\n",
    "        fp.write(\"%s %s\\n\" % (x, words_counts[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making vectorizer for learning SVM (on the base of words list)\n",
    "cv = CountVectorizer(vocabulary = words)\n",
    "vect_sent = cv.fit_transform(sent_p[0] + sent_m[0])\n",
    "vect_label = np.array(sent_p[1] + sent_m[1])\n",
    "\n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(vect_sent, vect_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_prep(sent):\n",
    "    #Normalization and stop-words delete from sent_list\n",
    "    sent_list_norm = []\n",
    "    sent_list = []\n",
    "    for x in sent:\n",
    "        \n",
    "        #Split into words\n",
    "        str_list = x.split()\n",
    "\n",
    "        #List from sentenses (words in list)\n",
    "        sent_list.append(str_list)\n",
    "    for sent in sent_list:\n",
    "        #print(sent)\n",
    "        one_sent = ''        \n",
    "        for x in sent[:-1]:\n",
    "                #print(x)\n",
    "                x_norm = norm(x)\n",
    "                if x_norm:                        \n",
    "                    if x_norm[1] not in ['PREP', 'PRCL', 'CONJ', 'NPRO']:\n",
    "                        one_sent = one_sent + ' ' + x_norm[0]\n",
    "        one_sent = one_sent.lstrip()\n",
    "        #print(one_sent)\n",
    "        sent_list_norm.append(one_sent)                \n",
    "    return sent_list_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def raw_cut(text):\n",
    "    #Delete digits    \n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"\", text)\n",
    "    #Split text into sentences and remove empty elements from list\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    sentences = list(filter(None, sentences))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter some text: Сегодня была отличная погода. Город погрузился в мрак и уныние, наступил зомби-апокалипсис. Давайте отпразднуем наши достижения! Нет повода не выпить текилы. Переговоры сорвались и несогласие сторон привело к вооруженному конфликту.\n",
      "Сегодня была отличная погода. 1\n",
      "Город погрузился в мрак и уныние, наступил зомби-апокалипсис. 1\n",
      "Давайте отпразднуем наши достижения! Нет повода не выпить текилы. 1\n",
      "Переговоры сорвались и несогласие сторон привело к вооруженному конфликту. -1\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Please enter some text: \")\n",
    "sentences = raw_cut(text)\n",
    "sent_list_norm = sent_prep(sentences)\n",
    "#cv = CountVectorizer(vocabulary = dic)\n",
    "vect_sent = cv.fit_transform(sent_list_norm)\n",
    "#print(vect_sent)\n",
    "vect_label_test = []\n",
    "\n",
    "for x in vect_sent:\n",
    "    vect_label_test.extend(clf.predict(x).tolist())\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i], vect_label_test[i])   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
