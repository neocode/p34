{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def norm(word):\n",
    "    \n",
    "    p = morph.parse(word)[0]\n",
    "    return p.normal_form, p.tag.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_prep1(sent):\n",
    "\n",
    "    #Normalization and stop-words delete from sent_list\n",
    "    sent_list_norm = []\n",
    "    label_list = []\n",
    "\n",
    "    sent_list = []\n",
    "    for x in sent:      \n",
    "        #Split into words\n",
    "        str_list = x.split()\n",
    "\n",
    "        #List from sentenses (words in list)\n",
    "        sent_list.append(str_list)\n",
    "\n",
    "\n",
    "    for sent in sent_list:\n",
    "        #print(sent)\n",
    "        one_sent = ''\n",
    "\n",
    "        if abs(float(sent[-1])) > 0.3:\n",
    "            for x in sent[:-1]:\n",
    "                #print(x)\n",
    "                x_norm = norm(x)        \n",
    "                if x_norm[1] not in ['PREP', 'PRCL', 'CONJ', 'NPRO']:\n",
    "                    one_sent = one_sent + ' ' + x_norm[0]                \n",
    "            weight = float(sent[-1])\n",
    "            if weight > 0:\n",
    "                weight_norm = 1\n",
    "            else:\n",
    "                weight_norm = -1\n",
    "            one_sent = one_sent.lstrip()\n",
    "            #print(one_sent)\n",
    "            sent_list_norm.append(one_sent)\n",
    "            label_list.append(weight_norm)\n",
    "    #print(sent_list_norm)\n",
    "    #print(label_list)\n",
    "    return sent_list_norm, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_prep2(sent):\n",
    "\n",
    "    #Normalization and stop-words delete from sent_list\n",
    "    sent_list_norm = []\n",
    "    sent_list = []\n",
    "    for x in sent:\n",
    "        \n",
    "        #Split into words\n",
    "        str_list = x.split()\n",
    "\n",
    "        #List from sentenses (words in list)\n",
    "        sent_list.append(str_list)\n",
    "    for sent in sent_list:\n",
    "        #print(sent)\n",
    "        one_sent = ''        \n",
    "        for x in sent[:-1]:\n",
    "                #print(x)\n",
    "                x_norm = norm(x)        \n",
    "                if x_norm[1] not in ['PREP', 'PRCL', 'CONJ', 'NPRO']:\n",
    "                    one_sent = one_sent + ' ' + x_norm[0]\n",
    "        one_sent = one_sent.lstrip()\n",
    "        #print(one_sent)\n",
    "        sent_list_norm.append(one_sent)                \n",
    "    return sent_list_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Make the dictionary from dic_ru file\n",
    "with open('dic_ru.txt') as f:\n",
    "    dic_list = f.readlines()\n",
    "dic_list = [x.strip('\\n') for x in dic_list]\n",
    "dic = []\n",
    "for x in dic_list:\n",
    "    str_list = x.split()\n",
    "    if len(str_list) == 3:\n",
    "        #print(str_list[0], str_list[2])        \n",
    "        #dic.append([str_list[0], float(str_list[2])])\n",
    "        dic.append(str_list[0])\n",
    "#print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ordered output to the file\n",
    "with open ('dic_learn_ru.txt', 'w') as fp:    \n",
    "    for x in dic:\n",
    "        fp.write(\"%s %s\\n\" % (dic.index(x), x))\n",
    "\n",
    "#Make the dictionary from sent_dic_ru file\n",
    "with open('sent_dic_ru.txt') as f:\n",
    "    sent = f.readlines()\n",
    "sent = [x.strip('\\n') for x in sent]                \n",
    "\n",
    "#print(sent)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_list_norm, label_list = sent_prep1(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(vocabulary = dic)\n",
    "vect_sent = cv.fit_transform(sent_list_norm)\n",
    "vect_label = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences for training: 75506\n"
     ]
    }
   ],
   "source": [
    "n_train = len(sent_list_norm)\n",
    "print('Number of sentences for training:', n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(vect_sent, vect_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_cut(text):\n",
    "\n",
    "    #Delete digits    \n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"\", text)\n",
    "\n",
    "    #Split text into sentences and remove empty elements from list\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    sentences = list(filter(None, sentences))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter some text: Ужасно быстро вирус распространился в среде социально-незащищенных слоев населения. Солнце светило: день был яркий и безоблачный.\n",
      "Ужасно быстро вирус распространился в среде социально-незащищенных слоев населения. -1\n",
      "Солнце светило: день был яркий и безоблачный. 1\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Please enter some text: \")\n",
    "sentences = raw_cut(text)\n",
    "sent_list_norm = sent_prep2(sentences)\n",
    "#cv = CountVectorizer(vocabulary = dic)\n",
    "vect_sent = cv.fit_transform(sent_list_norm)\n",
    "#print(vect_sent)\n",
    "vect_label_test = []\n",
    "\n",
    "for x in vect_sent:\n",
    "    vect_label_test.extend(clf.predict(x).tolist())\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i], vect_label_test[i])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
